# Set (uninformative) priors
a_mu_prior <- matrix(0, m) # Vector of prior parameter means
a_v_i_prior <- diag(0, m) # Inverse of the prior covariance matrix
u_sigma_df_prior <- 0 # Prior degrees of freedom
u_sigma_scale_prior <- diag(0, k) # Prior covariance matrix
u_sigma_df_post <- t + u_sigma_df_prior # Posterior degrees of freedom
# Initial values
u_sigma_i <- diag(.00001, k)
u_sigma <- solve(u_sigma_i)
# Data containers for posterior draws
draws_a <- matrix(NA, m, store)
draws_sigma <- matrix(NA, k^2, store)
# Start Gibbs sampler
for (draw in 1:iter) {
# Draw conditional mean parameters
a <- post_normal(y, x, u_sigma_i, a_mu_prior, a_v_i_prior)
# Draw variance-covariance matrix
u <- y - matrix(a, k) %*% x # Obtain residuals
u_sigma_scale_post <- solve(u_sigma_scale_prior + tcrossprod(u))
u_sigma_i <- matrix(rWishart(1, u_sigma_df_post, u_sigma_scale_post)[,, 1], k)
u_sigma <- solve(u_sigma_i) # Invert Sigma_i to obtain Sigma
# Store draws
if (draw > burnin) {
draws_a[, draw - burnin] <- a
draws_sigma[, draw - burnin] <- u_sigma
}
}
# After the Gibbs sampler has finished, obtain point estimates of the coefficient matrix by as the posterior mean
A <- rowMeans(draws_a) # Obtain means for every row
A <- matrix(A, k) # Transform mean vector into a matrix
A <- round(A, 3) # Round values
dimnames(A) <- list(dimnames(y)[[1]], dimnames(x)[[1]]) # Rename matrix dimensions
A # Print
# Analogously, obtain point estimates of the variance-covariance matrix as
Sigma <- rowMeans(draws_sigma) # Obtain means for every row
Sigma <- matrix(Sigma, k) # Transform mean vector into a matrix
Sigma <- round(Sigma * 10^4, 2) # Round values
dimnames(Sigma) <- list(dimnames(y)[[1]], dimnames(y)[[1]]) # Rename matrix dimensions
Sigma # Print
# create an object similar to that created by VAR() in vars package.
bvar_est <- bvar(y = y, x = x, A = draws_a[1:18,],
C = draws_a[19:21, ], Sigma = draws_sigma)
# Thin posterior draws
bvar_est <- thin(bvar_est, thin = 15)
# Forecast with function predict
bvar_pred <- predict(bvar_est, n.ahead = 10, new_D = rep(1, 10))
plot(bvar_pred)
# generate standard IRF
FEIR <- irf(bvar_est, impulse = "income", response = "cons", n.ahead = 8)
plot(FEIR, main = "Forecast Error Impulse Response", xlab = "Period", ylab = "Response")
# generate standard GIRF
GIR <- irf(bvar_est, impulse = "income", response = "cons", n.ahead = 8, type = "gir")
plot(GIR, main = "Generalised Impulse Response", xlab = "Period", ylab = "Response")
# Forecast error variance decomposition
# Default forecast error variance decomposition (FEVD) is based on orthogonalised impulse responses (OIR).
bvar_fevd_oir <- fevd(bvar_est, response = "cons")
plot(bvar_fevd_oir, main = "OIR-based FEVD of consumption")
graphics.off()
plot(bvar_pred)
par(mar=c(1,1,1,1))
A <- rowMeans(draws_a) # Obtain means for every row
A <- matrix(A, k) # Transform mean vector into a matrix
A <- round(A, 3) # Round values
dimnames(A) <- list(dimnames(y)[[1]], dimnames(x)[[1]]) # Rename matrix dimensions
A # Print
# Analogously, obtain point estimates of the variance-covariance matrix as
Sigma <- rowMeans(draws_sigma) # Obtain means for every row
Sigma <- matrix(Sigma, k) # Transform mean vector into a matrix
Sigma <- round(Sigma * 10^4, 2) # Round values
dimnames(Sigma) <- list(dimnames(y)[[1]], dimnames(y)[[1]]) # Rename matrix dimensions
Sigma # Print
# create an object similar to that created by VAR() in vars package.
bvar_est <- bvar(y = y, x = x, A = draws_a[1:18,],
C = draws_a[19:21, ], Sigma = draws_sigma)
# Thin posterior draws
bvar_est <- thin(bvar_est, thin = 15)
# Forecast with function predict
bvar_pred <- predict(bvar_est, n.ahead = 10, new_D = rep(1, 10))
par(mar=c(1,1,1,1))
plot(bvar_pred)
swdata <- ts(stockwatson[ ,c(2:4)], start = c(1960,1), frequency = 4)
View(swdata)
setwd("~/Desktop/Uppsala/U-Esami già dati /Financial Econometrics/FEcode/Exam_Feb19")
# Library
library(FE)
library(TTR,quantmod)
library(dlm)
library(tidyverse)
library(stats)
library(forecast)
library(knitr)
library(kableExtra)
library(gridExtra)
library(psych)
# install.packages("EnvStats")
library(EnvStats) # for Q-Q plot
library(YieldCurve)
# install.packages("sarima")
library(sarima)
library(aTSA)
library(rugarch)
# install.packages("fGarch")
library(fGarch)
library(tseries)
library(urca)
library(graphics)
library(MASS)
library(strucchange)
library(sandwich)
library(lmtest)
library(vars)
library(tsDyn)
library(fUnitRoots)
rm(list=ls())        # clean global environment
setwd("~/Desktop/Uppsala/U-Esami già dati /Financial Econometrics/FEcode/Exam_Feb19")
data <- read.table("Dividends.dat", header=T, stringsAsFactors=F)
data <- ts(data[,2:61], start = c(1966, 1), frequency = 4)
View(data)
# I build a simple function that computes the unweighted index
make_index <- function(series){
ind <- series/stats::lag(series)
ind <- apply(ind,1,mean) %>%
ts(start = c(1966, 1), frequency = 4)
return(ind)
}
make_index <- function(series){
ind <- apply(series,1,mean) %>%
ts(start = c(1966, 1), frequency = 4)
# ind <- (1 + ind/100) - 1
return(ind)
}
pindex   <- make_index(prices)
plot(pindex)
dpsindex <- make_index(dps)
epsindex <- make_index(eps)
# Log dividend (net) growth rate
div_growth <- (diff(log(dpsindex))) %>%
ts(start = c(1966, 2), frequency = 4)
# Log dividend-price ratio: "The dividend-price ratio is
# measured as the sum of dividends paid on the index the previous year,
# divided by the current level of the index" (Slide 25, Lecture 6, Bullet point 3)
# which is what Campbell, Lo, and MacKinley(1997) do
sum(dps,stats::lag(dps[,1], n=1L),stats::lag(dps, n=2L),stats::lag(dps, n=3L),stats::lag(dps, n=3L))  %>%
ts(start = c(1966, 5), frequency = 4)
head(prices[,3],5)
dp_ratio <- log(dpsindex/pindex) %>%
ts(start = c(1966, 1), frequency = 4)
# Earnings-price ratio
ep_ratio <- (epsindex/pindex) %>%
ts(start = c(1966, 1), frequency = 4)
par(mar=c(2,2,2,2))  # Regulate margins
par(mfrow=c(1,1))    # Reset environment
ggtsdisplay(div_growth, main="Log Dividend Growth Rate", points = F,
theme = theme_minimal())
ggtsdisplay(dp_ratio, main="Log Dividend-Price Ratio", points = F,
theme = theme_minimal())
ggtsdisplay(ep_ratio, main="Earnings-Price Ratio", points = F,
theme = theme_minimal())
rm(list=ls())        # clean global environment
setwd("~/Desktop/Uppsala/U-Esami già dati /Financial Econometrics/FEcode/Exam_Feb19")
data <- read.table("Dividends.dat", header=T, stringsAsFactors=F)
data <- ts(data[,2:61], start = c(1966, 1), frequency = 4)
dps <- data[,1:15]
eps <- data[,16:30]
prices <- data[,31:45]
# I build a simple function that computes the unweighted index
make_index <- function(series){
ind <- series/stats::lag(series)
ind <- apply(ind,1,mean) %>%
ts(start = c(1966, 1), frequency = 4)
return(ind)
}
make_index <- function(series){
ind <- apply(series,1,mean) %>%
ts(start = c(1966, 1), frequency = 4)
# ind <- (1 + ind/100) - 1
return(ind)
}
pindex   <- make_index(prices)
plot(pindex)
dpsindex <- make_index(dps)
epsindex <- make_index(eps)
# Log dividend (net) growth rate
div_growth <- (diff(log(dpsindex))) %>%
ts(start = c(1966, 2), frequency = 4)
# Log dividend-price ratio: "The dividend-price ratio is
# measured as the sum of dividends paid on the index the previous year,
# divided by the current level of the index" (Slide 25, Lecture 6, Bullet point 3)
# which is what Campbell, Lo, and MacKinley(1997) do
sum(dps,stats::lag(dps[,1], n=1L),stats::lag(dps, n=2L),stats::lag(dps, n=3L),stats::lag(dps, n=3L))  %>%
ts(start = c(1966, 5), frequency = 4)
head(prices[,3],5)
dp_ratio <- log(dpsindex/pindex) %>%
ts(start = c(1966, 1), frequency = 4)
# Earnings-price ratio
ep_ratio <- (epsindex/pindex) %>%
ts(start = c(1966, 1), frequency = 4)
par(mar=c(2,2,2,2))  # Regulate margins
par(mfrow=c(1,1))    # Reset environment
ggtsdisplay(div_growth, main="Log Dividend Growth Rate", points = F,
theme = theme_minimal())
ggtsdisplay(dp_ratio, main="Log Dividend-Price Ratio", points = F,
theme = theme_minimal())
ggtsdisplay(ep_ratio, main="Earnings-Price Ratio", points = F,
theme = theme_minimal())
par(mar=c(2,2,2,2))  # Regulate margins
par(mfrow=c(1,1))    # Reset environment
library(VARsignR)
#######
# Opening routine
setwd("~/Desktop/Software/Github/Notreadyforcommit/SVAR_with_sign_restrictions")
rm(list = ls())
##################
##
# Import data
stockwatson <- read.delim("sw2001.txt", header=TRUE)
swdata <- ts(stockwatson[ ,c(2:4)], start = c(1960,1), frequency = 4)
##############################
# Specify sign restrictions
#############################
# In vector -constr the first element is the sign restriction imposed on the variable whose shock I am looking to identify.
# Since the variables appear in the following order: Inflation, Unemployment, and Fed Funds, +3 says that
# a contractionary monetary policy shock makes the Fed Funds rate (weakly) higher for at least some quarters after the shock.
# -1 says that I impose a negative sign restriction on the response of inflation to the MP shock, based on the prediction from economic theory
# that the shock will push inflation down for at least some quarters after the shock.
# I impose no sign restriction on the response of the unemployment rate (variable 2 in the dataset).
constr <- c(+3,-1)
# KMIN and KMAX define the first and the last periods in the repsonses to which restrictions are applied.
# First, I estimate a BVAR model with a flat normal inverted-Wishart prior and identify structural monetary policy shock using Uhlig (2005) rejection method
model1 <- uhlig.reject(Y=swdata, nlags=12, draws=200, subdraws=200, nkeep=1000, KMIN=1,
KMAX=6, constrained=constr, constant=FALSE, steps=60)
# Alternatively, one can use Rubio-Ramirez et al. (2010) rejection method.
model2 <- rwz.reject(Y=uhligdata, nlags=12, draws=200, subdraws=200, nkeep=1000,
KMIN=1, KMAX=6, constrained=constr, constant=FALSE, steps=60)
# All commands below are valid also for model2.
# The MCMC sampler stops either teh desiered number of draws that satisfy all sign restrictions at the same time have drawn
# or when the maximum number of draws has been reached.
# BDraws: posterior draws of the coefficients of the model.
# SDraws: posterior draws of the variance-covariance matrix.
# IRFS: posterior draws for the impulse response functions.
# SHOCKS: posterior draes of the shock. Dimension of the array: T-nlags
summary(model1)
# Check the number of rejected draws. If you see many rejected draws, the model is poorly specified.
############################
# Bayesian VAR IRFs
############################
irfs1 <- model1$IRFS
labels <- c("Inflation","Unemployment","Federal Funds Rate")
# Plot the median of the impulse response draws and plot them, accompanied by confidence bands.
par(mar=c(2,2,2,2))  # Regulate margins
par(mfrow=c(1,1))    # Reset environment
irfplot(irfdraws=irfs1, type="median", labels=labels, save=FALSE, bands=c(0.16, 0.84),
grid=TRUE, bw=FALSE)
View(stockwatson)
View(stockwatson)
View(swdata)
#########################################################
# This implementation was written by Camilo Marchesini
########################################################
# This file uses the dataset by Stock and Watson (2001) to illustrate the workings of a SVAR where structural shocks
# are identified via sign restrictions.
# MIT License
#
# Copyright (c) 2019 Camilo Marchesini
#
# Permission is hereby granted, free of charge, to any person obtaining a copy
# of this software and associated documentation files (the "Software"), to deal
# in the Software without restriction, including without limitation the rights
# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
# copies of the Software, and to permit persons to whom the Software is
# furnished to do so, subject to the following conditions:
#
#   The above copyright notice and this permission notice shall be included in all
# copies or substantial portions of the Software.
#
# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
# SOFTWARE.
# SVAR with sign restrictions
# using package VARsignR
# Stock and Watson data.
# This package depends on other three packages
# install.packages("minqa")  # implement the UOBYQA algorithm to minimise Uhlig's penalty function
# install.packages("HI")     # Draws to create the orthogonal impulse vector alpha.
# install.packages("mvnfast")# Draws for the coefficient of the model from the multivariate normal.
# Install VARsignR
# install.packages("VARsignR")
# Library
library(VARsignR)
#######
# Opening routine
setwd("~/Desktop/Software/Github/Notreadyforcommit/SVAR_with_sign_restrictions")
rm(list = ls())
# Import data
stockwatson <- read.delim("sw2001.txt", header=TRUE)
swdata <- ts(stockwatson[ ,c(2:4)], start = c(1960,1), frequency = 4)
##############################
# Specify sign restrictions
#############################
# In vector -constr the first element is the sign restriction imposed on the variable whose shock I am looking to identify.
# Since the variables appear in the following order: Inflation, Unemployment, and Fed Funds, +3 says that
# a contractionary monetary policy shock makes the Fed Funds rate (weakly) higher for at least some quarters after the shock.
# -1 says that I impose a (weakly) negative sign restriction on the response of inflation to the MP shock, based on the prediction from economic theory
# that the shock will push inflation down for at least some quarters after the shock.
# I impose no sign restriction on the response of the unemployment rate (variable 2 in the dataset), but I could.
constr <- c(+3,-1)
# KMIN and KMAX define the first and the last periods in the repsonses to which restrictions are applied.
# First, I estimate a BVAR model with a flat normal inverted-Wishart prior and identify structural monetary policy shock using Uhlig (2005) rejection method
model_uhlig <- uhlig.reject(Y=swdata, nlags=12, draws=200, subdraws=200, nkeep=1000, KMIN=1,
KMAX=6, constrained=constr, constant=FALSE, steps=60)
# Alternatively, one can use Rubio-Ramirez et al. (2010) rejection method.
model_rwz<- rwz.reject(Y=uhligdata, nlags=12, draws=200, subdraws=200, nkeep=1000,
KMIN=1, KMAX=6, constrained=constr, constant=FALSE, steps=60)
# All commands below are valid also for model2.
# The MCMC sampler stops either teh desiered number of draws that satisfy all sign restrictions at the same time have drawn
# or when the maximum number of draws has been reached.
# BDraws: posterior draws of the coefficients of the model.
# SDraws: posterior draws of the variance-covariance matrix.
# IRFS: posterior draws for the impulse response functions.
# SHOCKS: posterior draes of the shock. Dimension of the array: T-nlags.
summary(model_uhlig)
# Check the number of rejected draws. If you see many rejected draws, the model is poorly specified.
############################
# Bayesian VAR IRFs
############################
irfs <- model_uhlig$IRFS
labels <- c("Inflation","Unemployment","Federal Funds Rate")
# Plot the median of the impulse response draws and plot them, accompanied by confidence bands.
par(mar=c(2,2,2,2))  # Regulate margins
par(mfrow=c(1,1))    # Reset environment
irfplot(irfdraws=irfs, type="median", labels=labels, save=FALSE, bands=c(0.16, 0.84),
grid=TRUE, bw=FALSE)
###############################################
# Plot forecast error variance decompositions
##############################################
# Extract FEVD
fevd1 <- model_uhlig$FEVDS
# Plot.
fevdplot(fevd1, label=labels, save=FALSE, bands=c(0.16, 0.84), grid=TRUE,
bw=FALSE, table=FALSE, periods=NULL)
# Show FEVD in table at different horizons.
fevd.table <- fevdplot(fevd1, table=TRUE, label=labels, periods=c(1,10,20,30,40,50,60))
# Print to screen.
print(fevd.table)
# Extract the identified time series of the structural shock to the interest rate.
#  I use the quantile function to generate confidence intervals for these shocks given the desired number of draws in nkeep.
# Confidence interevals stored as a time series object, where we lost the first 12 observations as a part of the estimation procedure.
shocks <- model_uhlig$SHOCKS
# Plot structural shock series.
ss <- ts(t(apply(shocks,2,quantile,probs=c(0.5, 0.16, 0.84))), frequency=12, start=c(1966,1))
plot(ss[,1], type="l", col="blue", ylab="Interest rate shock", ylim=c(min(ss), max(ss)))
abline(h=0, col="black")
# Add confidence bands.
lines(ss[,2], col="red")
lines(ss[,3], col="red")
##########################################
# Uhlig's (2005) penalty function method
##########################################
# Now I re-estimate the model using Uhlig's (2005) penalty function method.
model_penalty <- uhlig.penalty(Y=uhligdata, nlags=12, draws=2000, subdraws=1000,
nkeep=1000, KMIN=1, KMAX=6, constrained=constr,
constant=FALSE, steps=60, penalty=100, crit=0.001)
summary(model_penalty )
# Plot IRfs.
irfs3 <- model_penalty $IRFS
irfplot(irfdraws=irfs3, type="median", labels=labels, save=FALSE, bands=c(0.16, 0.84),
grid=TRUE, bw=FALSE)
###########################################
# Fry-Pagan (2011) Median-Target Method
############################################
# To see how well the shocks are identified, i.e. how well the model is specified,
# I compare the IRFs by uhlig.rejection to those by the FP method.
# If the IRFs produced by Uhlig's (2005) rejection method are far away from those produced by FP method, it would be evidence of
# model misspecification.
fp.target(Y=swdata, irfdraws=irfs1,  nlags=12,  constant=F, labels=labels, target=TRUE,
type="median", bands=c(0.16, 0.84), save=FALSE,  grid=TRUE, bw=FALSE,
legend=TRUE, maxit=1000)
# End.
setwd("~/SVAR_with_sign_restrictions")
setwd("~/Users/camilomarchesini/codes_github/SVAR_with_sign_restrictions")
setwd("~/codes_github/SVAR_with_sign_restrictions")
setwd("~/codes_github/SVAR_with_sign_restrictions")
#########################################################
# This implementation was written by Camilo Marchesini
########################################################
# This file uses the dataset by Stock and Watson (2001) to illustrate the workings of a SVAR where structural shocks
# are identified via sign restrictions.
# MIT License
#
# Copyright (c) 2019 Camilo Marchesini
#
# Permission is hereby granted, free of charge, to any person obtaining a copy
# of this software and associated documentation files (the "Software"), to deal
# in the Software without restriction, including without limitation the rights
# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
# copies of the Software, and to permit persons to whom the Software is
# furnished to do so, subject to the following conditions:
#
#   The above copyright notice and this permission notice shall be included in all
# copies or substantial portions of the Software.
#
# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
# SOFTWARE.
# SVAR with sign restrictions
# using package VARsignR
# Stock and Watson data.
# This package depends on other three packages
# install.packages("minqa")  # implement the UOBYQA algorithm to minimise Uhlig's penalty function
# install.packages("HI")     # Draws to create the orthogonal impulse vector alpha.
# install.packages("mvnfast")# Draws for the coefficient of the model from the multivariate normal.
# Install VARsignR
# install.packages("VARsignR")
# Library
library(VARsignR)
#######
# Opening routine
setwd("~/codes_github/SVAR_with_sign_restrictions")
rm(list = ls())
# Import data
stockwatson <- read.delim("sw2001.txt", header=TRUE)
swdata <- ts(stockwatson[ ,c(2:4)], start = c(1960,1), frequency = 4)
##############################
# Specify sign restrictions
#############################
# In vector -constr the first element is the sign restriction imposed on the variable whose shock I am looking to identify.
# Since the variables appear in the following order: Inflation, Unemployment, and Fed Funds, +3 says that
# a contractionary monetary policy shock makes the Fed Funds rate (weakly) higher for at least some quarters after the shock.
# -1 says that I impose a (weakly) negative sign restriction on the response of inflation to the MP shock, based on the prediction from economic theory
# that the shock will push inflation down for at least some quarters after the shock.
# I impose no sign restriction on the response of the unemployment rate (variable 2 in the dataset), but I could.
constr <- c(+3,-1)
# KMIN and KMAX define the first and the last periods in the repsonses to which restrictions are applied.
# First, I estimate a BVAR model with a flat normal inverted-Wishart prior and identify structural monetary policy shock using Uhlig (2005) rejection method
model_uhlig <- uhlig.reject(Y=swdata, nlags=12, draws=200, subdraws=200, nkeep=1000, KMIN=1,
KMAX=6, constrained=constr, constant=FALSE, steps=60)
# Alternatively, one can use Rubio-Ramirez et al. (2010) rejection method.
model_rwz<- rwz.reject(Y=uhligdata, nlags=12, draws=200, subdraws=200, nkeep=1000,
KMIN=1, KMAX=6, constrained=constr, constant=FALSE, steps=60)
# All commands below are valid also for model2.
# The MCMC sampler stops either teh desiered number of draws that satisfy all sign restrictions at the same time have drawn
# or when the maximum number of draws has been reached.
# BDraws: posterior draws of the coefficients of the model.
# SDraws: posterior draws of the variance-covariance matrix.
# IRFS: posterior draws for the impulse response functions.
# SHOCKS: posterior draes of the shock. Dimension of the array: T-nlags.
summary(model_uhlig)
# Check the number of rejected draws. If you see many rejected draws, the model is poorly specified.
############################
# Bayesian VAR IRFs
############################
irfs <- model_uhlig$IRFS
labels <- c("Inflation","Unemployment","Federal Funds Rate")
# Plot the median of the impulse response draws and plot them, accompanied by confidence bands.
par(mar=c(2,2,2,2))  # Regulate margins
par(mfrow=c(1,1))    # Reset environment
irfplot(irfdraws=irfs, type="median", labels=labels, save=FALSE, bands=c(0.16, 0.84),
grid=TRUE, bw=FALSE)
###############################################
# Plot forecast error variance decompositions
##############################################
# Extract FEVD
fevd1 <- model_uhlig$FEVDS
# Plot.
fevdplot(fevd1, label=labels, save=FALSE, bands=c(0.16, 0.84), grid=TRUE,
bw=FALSE, table=FALSE, periods=NULL)
# Show FEVD in table at different horizons.
fevd.table <- fevdplot(fevd1, table=TRUE, label=labels, periods=c(1,10,20,30,40,50,60))
# Print to screen.
print(fevd.table)
# Extract the identified time series of the structural shock to the interest rate.
#  I use the quantile function to generate confidence intervals for these shocks given the desired number of draws in nkeep.
# Confidence interevals stored as a time series object, where we lost the first 12 observations as a part of the estimation procedure.
shocks <- model_uhlig$SHOCKS
# Plot structural shock series.
ss <- ts(t(apply(shocks,2,quantile,probs=c(0.5, 0.16, 0.84))), frequency=12, start=c(1966,1))
plot(ss[,1], type="l", col="blue", ylab="Interest rate shock", ylim=c(min(ss), max(ss)))
abline(h=0, col="black")
# Add confidence bands.
lines(ss[,2], col="red")
lines(ss[,3], col="red")
##########################################
# Uhlig's (2005) penalty function method
##########################################
# Now I re-estimate the model using Uhlig's (2005) penalty function method.
model_penalty <- uhlig.penalty(Y=uhligdata, nlags=12, draws=2000, subdraws=1000,
nkeep=1000, KMIN=1, KMAX=6, constrained=constr,
constant=FALSE, steps=60, penalty=100, crit=0.001)
summary(model_penalty )
# Plot IRfs.
irfs3 <- model_penalty $IRFS
irfplot(irfdraws=irfs3, type="median", labels=labels, save=FALSE, bands=c(0.16, 0.84),
grid=TRUE, bw=FALSE)
###########################################
# Fry-Pagan (2011) Median-Target Method
############################################
# To see how well the shocks are identified, i.e. how well the model is specified,
# I compare the IRFs by uhlig.rejection to those by the FP method.
# If the IRFs produced by Uhlig's (2005) rejection method are far away from those produced by FP method, it would be evidence of
# model misspecification.
fp.target(Y=swdata, irfdraws=irfs1,  nlags=12,  constant=F, labels=labels, target=TRUE,
type="median", bands=c(0.16, 0.84), save=FALSE,  grid=TRUE, bw=FALSE,
legend=TRUE, maxit=1000)
# End.
